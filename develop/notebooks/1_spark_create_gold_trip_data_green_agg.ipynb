{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Trip Data Aggregation \n",
        "### Group By Columns\n",
        "1. year\n",
        "2. Month\n",
        "3. Pickup Location ID\n",
        "4. Drop Off Location ID\n",
        "\n",
        "### Aggregated Columns\n",
        "1. Total Trip Count\n",
        "2. Total Fare Amount\n",
        "\n",
        "### Purpose of the notebook\n",
        "\n",
        "Demonstrate the integration between Spark Pool and Serverless SQL Pool\n",
        "\n",
        "1. Create the aggregated table in Spark Pool\n",
        "2. Access the data from Serverless SQL Pool "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Set the folder paths so that it can be used later. \n",
        "bronze_folder_path = 'abfss://nyc-taxi-data@synapsecoursedlnikolas.dfs.core.windows.net/raw'\n",
        "silver_folder_path = 'abfss://nyc-taxi-data@synapsecoursedlnikolas.dfs.core.windows.net/silver'\n",
        "gold_folder_path = 'abfss://nyc-taxi-data@synapsecoursedlnikolas.dfs.core.windows.net/gold'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Set the spark config to be able to get the partitioned columns year and month as strings rather than integers\n",
        "spark.conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "%%sql\n",
        "\n",
        "CREATE DATABASE IF NOT EXISTS nyc_taxi_ldw_spark\n",
        "LOCATION 'abfss://nyc-taxi-data@synapsecoursedlnikolas.dfs.core.windows.net/gold';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.functions import col, when, sum, to_date\n",
        "\n",
        "\n",
        "# Load the data from the views\n",
        "trip_data_green_df = spark.read.parquet(f\"{silver_folder_path}/trip_data_green\") \n",
        "taxi_zone_df = spark.read.parquet(f\"{silver_folder_path}/taxi_zone\")\n",
        "calendar_df = spark.read.parquet(f\"{silver_folder_path}/calendar\")\n",
        "payment_type_df = spark.read.parquet(f\"{silver_folder_path}/payment_type\")\n",
        "\n",
        "# Perform the join operations\n",
        "result_df = trip_data_green_df.alias(\"td\") \\\n",
        "    .join(taxi_zone_df.alias(\"tz\"), col(\"td.pu_location_id\") == col(\"tz.location_id\")) \\\n",
        "    .join(calendar_df.alias(\"cal\"), to_date(col(\"td.lpep_pickup_datetime\")) == col(\"cal.date\")) \\\n",
        "    .join(payment_type_df.alias(\"pt\"), col(\"td.payment_type\") == col(\"pt.payment_type\")) \\\n",
        "    .groupBy(\n",
        "        col(\"td.year\"),\n",
        "        col(\"td.month\"),\n",
        "        col(\"tz.borough\"),\n",
        "        to_date(col(\"td.lpep_pickup_datetime\")).alias(\"trip_date\"),\n",
        "        col(\"cal.day_name\")\n",
        "    ) \\\n",
        "    .agg(\n",
        "        sum(when(col(\"pt.description\") == 'Credit card', 1).otherwise(0)).alias(\"card_trip_count\"),\n",
        "        sum(when(col(\"pt.description\") == 'Cash', 1).otherwise(0)).alias(\"cash_trip_count\"),\n",
        "        when(col(\"cal.day_name\").isin('Saturday', 'Sunday'), 'Y').otherwise('N').alias(\"trip_day_weekend_ind\")\n",
        "    )\n",
        "\n",
        "# Ensure the DataFrame has the necessary columns for writing\n",
        "trip_data_green_agg_df = result_df.select(\n",
        "    col(\"year\"),\n",
        "    col(\"month\"),\n",
        "    col(\"borough\"),\n",
        "    col(\"trip_date\"),\n",
        "    col(\"day_name\").alias(\"trip_day\"),\n",
        "    col(\"trip_day_weekend_ind\"),\n",
        "    col(\"card_trip_count\"),\n",
        "    col(\"cash_trip_count\")\n",
        ")\n",
        "\n",
        "# Write the aggregated DataFrame to a table\n",
        "trip_data_green_agg_df.write.mode(\"overwrite\") \\\n",
        "    .partitionBy(\"year\", \"month\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .saveAsTable(\"nyc_taxi_ldw_spark.trip_data_green_agg\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}